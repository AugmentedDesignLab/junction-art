{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recent-bunch",
   "metadata": {},
   "source": [
    "# SAC (Soft Actor-Critic)\n",
    "\n",
    "SAC (Soft Actor-Critic) is one of today's popular algorithm, which is based on **off-policy** DDPG discussed in [here](./05-ddpg.ipynb).<br>\n",
    "However, unlike DDPG, SAC applies entropy regularization and trains a stochastic policy, not a deterministic policy.\n",
    "\n",
    "Entropy is defined as $ H(P) = -\\int P(x) \\log P(x) = E_x[-\\log P(x)] $ and it means how $ P(\\cdot) $ is distributed intuitively.<br>\n",
    "For instance, if it has 8 possible states, each of which is equally likely in discrete distribution, it will have $ H(P) = -\\sum P(x) \\log P(x) = -8 \\times \\frac{1}{8} \\log_2 \\frac{1}{8} = 3 $. (This implies that it needs 3 bits evenly.) If the distribution is $ (\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}) $, it will have $ H(P) = -\\frac{1}{2} \\log_2 \\frac{1}{2} - \\frac{1}{4} \\log_2 \\frac{1}{4} - \\frac{1}{8} \\log_2 \\frac{1}{8} - \\frac{1}{16} \\log_2 \\frac{1}{16} - 4 \\times \\frac{1}{64} \\log_2 \\frac{1}{64} = 2 $.<br>\n",
    "(Note that, for simplicity, I have replaced the base e of logarithm with 2.)<br>\n",
    "\n",
    "As you can see above, entropy will be larger, when the distribution has much randomness.\n",
    "\n",
    "> Note : In continuous distribution, it's known that the distribution that maximizes the entropy is Gaussian distribution. Here I don't go into details, but KL-divergence (the penalty for large updates) discussed in [PPO](./04-ppo.ipynb) is closely related with this entropy term.\n",
    "\n",
    "In SAC, instead of using a reward expectation $ r_t + \\gamma (d_t - 1) Q_{{\\phi}^{\\prime}} $ used in DDPG, it applies $ r_t + \\gamma (d_t - 1) (Q_{{\\phi}^{\\prime}} + \\alpha H(P)) $ (where $\\alpha$ is a coefficient parameter for entropy weight, called entropy temperature) in order to balance between exploitation and exploration.<br>\n",
    "Even if the estimated Q-value increases, it might be rejected when the entropy is largely reduced.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/reinforcement-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4afbae-8d30-48a7-87aa-e18ae9667883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa81765d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# import gym \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126dd724",
   "metadata": {},
   "source": [
    "Unlike [DDPG](./05-ddpg.ipynb), we can use discrete action space in SAC. (See below for this reason.)<br>\n",
    "We then now use standard CartPole agent in Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385715b",
   "metadata": {},
   "source": [
    "Unlike DDPG, SAC trains a stochastic policy $ \\pi_{\\theta}(\\cdot) $ (where $ \\theta $ is parameters) instead of a deterministic policy $ \\mu_{\\theta}(\\cdot) $. (And we don't use target policy network $ \\pi_{\\theta^{\\prime}} $.)<br>\n",
    "In this example, I use categorical distribution (same as, used in [policy gradient](./02-policy-gradient.ipynb) and [PPO](./04-ppo.ipynb) example) for a policy $ P(\\cdot | \\pi_\\theta(s)) $, because it's discrete action space:\n",
    "\n",
    "> Note : For the bounded continuous action space between $ l $ and $ h $, use Gaussian distribution as follows.<br>\n",
    "> $ P(\\cdot | \\pi_\\theta(s)) = ((tanh(\\mathcal{N}(\\mu_{\\theta}(s), \\sigma_{\\theta}(s))) + 1.0) / 2.0) \\times (h - l) + l  $\n",
    "\n",
    "Because we use a stochastic policy, we don't then need Ornstein-Uhlenbeck noise used in [DDPG](./05-ddpg.ipynb) any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4c8dbe81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Policy net (pi_theta)\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, state_dim = 4, nActions = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(state_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 2*nActions) # 2 * number + 1\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        return outs\n",
    "\n",
    "# pi_model = PolicyNet(nActions=20).to(device)\n",
    "\n",
    "# Pick up action (for each step in episode)\n",
    "def pick_sample(s, pi_model):\n",
    "    with torch.no_grad():\n",
    "        #   --> size : (1, 4)\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        # Get logits from state\n",
    "        #   --> size : (1, 2)\n",
    "        logits = pi_model(s_batch)\n",
    "        #   --> size : (2)\n",
    "        logits = logits.squeeze(dim=0)\n",
    "        # From logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Pick up action's sample\n",
    "        #   --> size : (1)\n",
    "        a = torch.multinomial(probs, num_samples=1)\n",
    "        #   --> size : ()\n",
    "        a = a.squeeze(dim=0)\n",
    "        # Return\n",
    "        return a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845f8cf",
   "metadata": {},
   "source": [
    "Same as clipped double-Q (twin-Q) DDPG method (see the latter part in [here](./05-ddpg.ipynb)), we use 2 Q-networks - $ Q_{\\phi_1}(s), Q_{\\phi_2}(s) $ - and corresponding 2 target networks - $ Q_{\\phi_1^{\\prime}}(s), Q_{\\phi_2^{\\prime}}(s) $.\n",
    "\n",
    "You will find that this is different from the one used in [DDPG](./05-ddpg.ipynb). (In DDPG, we have used $Q(s, a)$.)<br>\n",
    "For categorical distribution with the depth n (in this example, n=2), the output of $ Q(\\cdot) $ is n-dimensional tensor, in which each element represents the expectation of Q-value for the corresponding action. And we then use $ Q(s) \\cdot \\tilde{a} $ (i.e, dot product operation) instead of $ Q(s, a) $, where $ \\tilde{a} $ is one hot tensor for action $ a $.<br>\n",
    "Because of this reason, we use $Q(s)$ instead of $Q(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2b916fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim = 4, hidden_dim=64, nActions = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(state_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 2*nActions) # 2 * number + 1\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        return outs\n",
    "\n",
    "# q_origin_model1 = QNet(nActions=20).to(device)  # Q_phi1\n",
    "# q_origin_model2 = QNet(nActions=20).to(device)  # Q_phi2\n",
    "# q_target_model1 = QNet(nActions=20).to(device)  # Q_phi1'\n",
    "# q_target_model2 = QNet(nActions=20).to(device)  # Q_phi2'\n",
    "# _ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "# _ = q_target_model2.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9c1b5",
   "metadata": {},
   "source": [
    "As we saw in [clipped double-Q DDPG](./05-ddpg.ipynb), we optimize policy parameter $ \\theta $ to maximize $ Q_{\\phi_1}(s, a^*) + \\alpha H(P(\\cdot | \\pi_\\theta(s))) $ where $ a^* $ is an optimal action.\n",
    "\n",
    "As I have mentioned above, $ H(P) = E_x[-\\log P(x)] $.<br>\n",
    "In this categorical distribution (in discrete action space), $ H $ will then be the following dot product :\n",
    "\n",
    "$ H(P) = H(P(\\cdot | \\pi_\\theta(s))) = -\\pi_\\theta(s) \\cdot \\log \\pi_\\theta(s) $\n",
    "\n",
    "where $ \\pi_\\theta(s) $ is one hot probability.\n",
    "\n",
    "For $ Q(s, a^*) $ term, it will become the following dot product. (See above for this reason.) :\n",
    "\n",
    "$ Q_{\\phi_1}(s, a^*) = Q_{\\phi_1}(s) \\cdot \\pi_\\theta(s) $\n",
    "\n",
    "To summarize, we should optimize $ \\theta $ to maximize :\n",
    "\n",
    "$ E\\left[ \\pi_\\theta(s) \\cdot Q_{\\phi_1}(s) - \\alpha \\pi_\\theta(s) \\cdot \\log \\pi_\\theta(s) \\right] = E\\left[ \\pi_\\theta(s) \\cdot (Q_{\\phi_1}(s) - \\alpha \\log \\pi_\\theta(s)) \\right] $\n",
    "\n",
    "> Note : Here I have used a constant $ \\alpha $, but the appropriate temperature ($ \\alpha $) depends on the magnitude of rewards, and it's not so easy to determine appropriate temprature, because it also depends on policy, which improves over time during training.<br>\n",
    "> There exists a variation of SAC, in which $ \\alpha $ is also learned over the course of training to align to appropriate entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "af62799c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "# alpha = 0.1\n",
    "\n",
    "\n",
    "class categorical:\n",
    "    def __init__(self, s):\n",
    "        logits = pi_model(s)\n",
    "        self._prob = F.softmax(logits, dim=-1)\n",
    "        self._logp = torch.log(self._prob)\n",
    "\n",
    "    # probability (sum is 1.0) : P\n",
    "    def prob(self):\n",
    "        return self._prob\n",
    "\n",
    "    # log probability : log P()\n",
    "    def logp(self):\n",
    "        return self._logp\n",
    "\n",
    "def optimize_theta(states, alpha):\n",
    "    # Convert to tensor\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    # Disable grad in q_origin_model1 before computation\n",
    "    # (or use q_value.detach() not to include in graph)\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = False\n",
    "    # Optimize\n",
    "    opt_pi.zero_grad()\n",
    "    dist = categorical(states)\n",
    "    q_value = q_origin_model1(states)\n",
    "    term1 = dist.prob()\n",
    "    # alpha = log_alpha.exp().detach()\n",
    "    term2 = q_value - alpha * dist.logp()\n",
    "    # print(term1.shape, term2.shape)\n",
    "    # return\n",
    "    expectation = term1.unsqueeze(dim=1) @ term2.unsqueeze(dim=2)\n",
    "    expectation = expectation.squeeze(dim=1)\n",
    "    (-expectation).sum().backward()\n",
    "    opt_pi.step()\n",
    "    # Enable grad again\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "def optimize_alpha(states):\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    # Disable grad in q_origin_model1 before computation\n",
    "    # (or use q_value.detach() not to include in graph)\n",
    "    for p in pi_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dist = categorical(states)\n",
    "    alpha_optimizer.zero_grad()\n",
    "    alphaLoss = - (log_alpha.exp() * ((dist.logp() * dist.prob()).sum() - targetEntropy)).mean()\n",
    "    alphaLoss.backward()\n",
    "    alpha_optimizer.step()\n",
    "    alpha = log_alpha.exp().detach()\n",
    "    for p in pi_model.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120612ed",
   "metadata": {},
   "source": [
    "Same as we saw in [clipped double-Q DDPG](./05-ddpg.ipynb), we optimize parameter $ \\phi_1, \\phi_2 $ as follows :\n",
    "\n",
    "- Optimize $ \\phi_1 $ to minimize $ E\\left[ \\left( Q_{\\phi_1}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\left( \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) + \\alpha H(P(\\cdot | \\pi_\\theta(s_{t+1}))) \\right) \\right) \\right)^2 \\right] $\n",
    "- Optimize $ \\phi_2 $ to minimize $ E\\left[ \\left( Q_{\\phi_2}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\left( \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) + \\alpha H(P(\\cdot | \\pi_\\theta(s_{t+1}))) \\right) \\right) \\right)^2 \\right] $\n",
    "\n",
    "in which :\n",
    "\n",
    "- $ Q_{\\phi_i}(s_t, a_t) = Q_{\\phi_i}(s_t) \\cdot \\tilde{a_t} $ where $ \\tilde{a_t} $ is one hot vector of $ a_t $\n",
    "- $ Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) =  Q_{\\phi_i^{\\prime}}(s_{t+1}) \\cdot \\pi_\\theta(s_{t+1}) $ where $ \\pi_\\theta(s_{t+1}) $ is one hot probability\n",
    "- $ H(P(\\cdot | \\pi_\\theta(s_{t+1}))) = -\\pi_\\theta(s_{t+1}) \\cdot \\log \\pi_\\theta(s_{t+1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5fbac98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "\n",
    "def optimize_phi(states, actions, rewards, next_states, dones, alpha, nActions=20):\n",
    "    #\n",
    "    # Convert to tensor\n",
    "    #\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    #\n",
    "    # Compute r + gamma * (1 - d) (min Q(s_next,a_next') + alpha * H(P))\n",
    "    #\n",
    "    # alpha = log_alpha.exp().detach()\n",
    "    with torch.no_grad():\n",
    "        # min Q(s_next,a_next')\n",
    "        q1_tgt_next = q_target_model1(next_states)\n",
    "        q2_tgt_next = q_target_model2(next_states)\n",
    "        dist_next = categorical(next_states)\n",
    "        q1_target = q1_tgt_next.unsqueeze(dim=1) @ dist_next.prob().unsqueeze(dim=2)\n",
    "        q1_target = q1_target.squeeze(dim=1)\n",
    "        q2_target = q2_tgt_next.unsqueeze(dim=1) @ dist_next.prob().unsqueeze(dim=2)\n",
    "        q2_target = q2_target.squeeze(dim=1)\n",
    "        q_target_min = torch.minimum(q1_target, q2_target)\n",
    "        # alpha * H(P)\n",
    "        h = dist_next.prob().unsqueeze(dim=1) @ dist_next.logp().unsqueeze(dim=2)\n",
    "        h = h.squeeze(dim=1)\n",
    "        h = -alpha * h\n",
    "        # total\n",
    "        term2 = rewards + gamma * (1.0 - dones) * (q_target_min + h)\n",
    "\n",
    "    #\n",
    "    # Optimize critic loss for Q-network1\n",
    "    #\n",
    "    opt_q1.zero_grad()\n",
    "    one_hot_actions = F.one_hot(actions, num_classes=2*nActions).float()\n",
    "    q_value1 = q_origin_model1(states)\n",
    "    term1 = q_value1.unsqueeze(dim=1) @ one_hot_actions.unsqueeze(dim=2)\n",
    "    term1 = term1.squeeze(dim=1)\n",
    "    loss_q1 = F.mse_loss(\n",
    "        term1,\n",
    "        term2,\n",
    "        reduction=\"none\")\n",
    "    loss_q1.sum().backward()\n",
    "    opt_q1.step()\n",
    "\n",
    "    #\n",
    "    # Optimize critic loss for Q-network2\n",
    "    #\n",
    "    opt_q2.zero_grad()\n",
    "    one_hot_actions = F.one_hot(actions, num_classes=2*nActions).float()\n",
    "    q_value2 = q_origin_model2(states)\n",
    "    term1 = q_value2.unsqueeze(dim=1) @ one_hot_actions.unsqueeze(dim=2)\n",
    "    term1 = term1.squeeze(dim=1)\n",
    "    loss_q2 = F.mse_loss(\n",
    "        term1,\n",
    "        term2,\n",
    "        reduction=\"none\")\n",
    "    loss_q2.sum().backward()\n",
    "    opt_q2.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8100e",
   "metadata": {},
   "source": [
    "As we saw in [clipped double-Q DDPG](./05-ddpg.ipynb), target parameters $\\phi_1^{\\prime}, \\phi_2^{\\prime}$ are delayed with coefficient parameter (hyper-parameter) $ \\tau $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bb910b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = 0.002\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(q_origin_model1.parameters(), q_target_model1.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(q_origin_model2.parameters(), q_target_model2.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1f7f1",
   "metadata": {},
   "source": [
    "As we saw in [DDPG](./05-ddpg.ipynb), we use replay buffer to prevent from learning only for recent experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1172351c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) > self._next_idx:\n",
    "            self.buffer[self._next_idx] = item\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "        if self._next_idx == self.buffer_size - 1:\n",
    "            self._next_idx = 0\n",
    "        else:\n",
    "            self._next_idx = self._next_idx + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = [random.randint(0, len(self.buffer) - 1) for _ in range(batch_size)]\n",
    "        states   = [self.buffer[i][0] for i in indices]\n",
    "        actions  = [self.buffer[i][1] for i in indices]\n",
    "        rewards  = [self.buffer[i][2] for i in indices]\n",
    "        n_states = [self.buffer[i][3] for i in indices]\n",
    "        dones    = [self.buffer[i][4] for i in indices]\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "buffer = replayBuffer(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b19a01",
   "metadata": {},
   "source": [
    "Now let's put it all together !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635b7624-204c-48a6-b4fb-1e6707854864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inputTestCases/_input2ways_n=4_.pickle', 'rb') as f:\n",
    "    roadDefs = pickle.load(f) # deserialize using load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fe030199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from junctionart.roundabout.encodingGFN.setGenerationEnv import SetGenerationEnv\n",
    "size = 4\n",
    "nActions = 30\n",
    "\n",
    "# # models\n",
    "# pi_model = PolicyNet(state_dim=size, nActions=nActions).to(device)\n",
    "# q_origin_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1\n",
    "# q_origin_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2\n",
    "# q_target_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1'\n",
    "# q_target_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2'\n",
    "# _ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "# _ = q_target_model2.requires_grad_(False)  # target model doen't need grad\n",
    "# buffer = replayBuffer(20000)\n",
    "\n",
    "# # optimizers\n",
    "# opt_pi = torch.optim.AdamW(pi_model.parameters(), lr=0.0005)\n",
    "# opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "# opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "\n",
    "def train(env, nIter = 6000, batch_size = 250, disableBar = False):\n",
    "    for i in tqdm(range(nIter), disable = disableBar):\n",
    "        # Run episode till done\n",
    "        s = torch.zeros(1, size)\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        while not done:\n",
    "            a = pick_sample((s/nActions).squeeze().tolist(), pi_model)\n",
    "\n",
    "            s_next = env.update(s, torch.tensor([a]), inPlace = False)\n",
    "            \n",
    "            done = (s_next != 0).all().item()\n",
    "            if done:\n",
    "                config = (s_next.squeeze() - 1).long().tolist()\n",
    "                r = 10**env.getProxyReward(config, normalize=True)\n",
    "         \n",
    "            else:\n",
    "                r = 0\n",
    "            buffer.add([(s/nActions).squeeze().tolist(), a, r, (s_next/nActions).squeeze().tolist(), float(done)])\n",
    "            cum_reward += r\n",
    "            if buffer.length() >= 3000:\n",
    "                states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
    "                optimize_theta(states, alpha)\n",
    "                # optimize_alpha(states)\n",
    "                \n",
    "                optimize_phi(states, actions, rewards, n_states, dones, alpha, nActions=nActions)\n",
    "                update_target()\n",
    "            s = s_next\n",
    "        \n",
    "            # alpha = log_alpha.exp().detach()\n",
    "        print(\"Run episode{} with rewards {} s {} ALPHA {}\".format(i, cum_reward, s.squeeze().tolist(), alpha), end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0d585f15-a27f-4341-8bb3-7050d631447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def sampleRewardWithConfig(nIter, setEnv, pi_model):\n",
    "    rewardWithConfigs = []\n",
    "    for i in tqdm(range(nIter)):\n",
    "        # Run episode till done\n",
    "        s = torch.zeros(1, size)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = pick_sample((s/nActions).squeeze().tolist(), pi_model)\n",
    "\n",
    "            s_next = env.update(s, torch.tensor([a]))\n",
    "                 \n",
    "            done = (s_next != 0).all().item()\n",
    "            \n",
    "            if done:\n",
    "                config = (s_next.squeeze() - 1).long().tolist()\n",
    "                r = env.getProxyReward(config, normalize=True) \n",
    "           \n",
    "                rewardWithConfigs.append((r, config))\n",
    "            s = s_next\n",
    "    return rewardWithConfigs\n",
    "\n",
    "def getTopK(rewardWithConfigs, K):\n",
    "    modes = []\n",
    "    proxyRewards = []\n",
    "    \n",
    "    rewardWithConfigs.sort(key = lambda x : x[0], reverse=True)\n",
    "\n",
    "    for reward, config in rewardWithConfigs[:K]: # top-500 samples\n",
    "        modes.append(config)\n",
    "        proxyRewards.append(reward)\n",
    "\n",
    "    \n",
    "    return modes, proxyRewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3b3a33a2-3b57-4a2d-8ebd-60afc96cf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from junctionart.roundabout.encodingGFN.RoundaboutLaneEncodingEnv import RoundaboutLaneEncodingEnv\n",
    "from junctionart.roundabout.RewardUtil import RewardUtil\n",
    "\n",
    "def getRoundabouts(roadDefinition, modes):\n",
    "    env = RoundaboutLaneEncodingEnv()\n",
    "    roundabouts = []\n",
    "    for i in tqdm(range(len(modes))):\n",
    "        env.generateWithRoadDefinition(\n",
    "            roadDefinition=roadDefinition,\n",
    "            outgoingLanesMerge=False,\n",
    "            nSegments=nActions,\n",
    "            laneToCircularId=modes[i]\n",
    "        )\n",
    "        roundabouts.append(env.getRoundabout())\n",
    "    return roundabouts\n",
    "\n",
    "def getRewards(roundabouts):\n",
    "    rewards = [roundabout.getReward() for roundabout in roundabouts]\n",
    "    return rewards\n",
    "\n",
    "def getDiversityScore(roundabouts):\n",
    "    distances = []\n",
    "    for i in tqdm(range(len(roundabouts))):\n",
    "        for j in range(i + 1, len(roundabouts)):\n",
    "            distance = RewardUtil.getDistance(roundabouts[i], roundabouts[j])\n",
    "            distances.append(distance)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    return distances.sum() / (len(roundabouts) * (len(roundabouts) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8f6776db-a428-4def-821c-6738b4654a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:49<00:00, 34.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 462.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:50<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:48<00:00, 35.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 463.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:50<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:57<00:00, 29.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:22<00:00, 437.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [04:00<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:50<00:00, 33.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 471.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:48<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:46<00:00, 36.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:20<00:00, 486.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:18<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:59<00:00, 28.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:23<00:00, 417.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:50<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [01:25<00:00, 19.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 461.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:50<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:43<00:00, 39.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:17<00:00, 568.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:42<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:42<00:00, 39.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:17<00:00, 575.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:02<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:33<00:00, 50.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 799.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:31<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:35<00:00, 47.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 786.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [02:50<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:32<00:00, 51.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 791.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:52<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:34<00:00, 49.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 784.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:31<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:33<00:00, 50.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 799.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:07<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:40<00:00, 42.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:17<00:00, 581.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:09<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:44<00:00, 38.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 731.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:30<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:35<00:00, 47.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:15<00:00, 653.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:40<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:35<00:00, 48.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 766.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:33<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:40<00:00, 41.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:18<00:00, 538.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:06<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 1700/1700 [00:42<00:00, 40.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:16<00:00, 611.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:39<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scoresList = []\n",
    "diversityScores = []\n",
    "allRoundabouts = []\n",
    "output = {\"roundabouts\" : [], \"modes\" : [], \"proxyRewards\" : []}\n",
    "alpha = 0.4\n",
    "\n",
    "for roadDefinition in roadDefs:\n",
    "    env = SetGenerationEnv(size, nActions, roadDefinition)\n",
    "\n",
    "    targetEntropy = -nActions\n",
    "    log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "    # models\n",
    "    pi_model = PolicyNet(state_dim=size, nActions=nActions).to(device)\n",
    "    q_origin_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1\n",
    "    q_origin_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2\n",
    "    q_target_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1'\n",
    "    q_target_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2'\n",
    "    _ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "    _ = q_target_model2.requires_grad_(False)  # target model doen't need grad\n",
    "    buffer = replayBuffer(20000)\n",
    "\n",
    "    # optimizers\n",
    "    opt_pi = torch.optim.AdamW(pi_model.parameters(), lr=0.0005)\n",
    "    opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "    opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "    alpha_optimizer = torch.optim.AdamW(params=[log_alpha], lr=0.0005) \n",
    "    doneTraining = False\n",
    "    while not doneTraining:\n",
    "        try:\n",
    "            train(env, nIter=1700, batch_size=256)\n",
    "            doneTraining = True\n",
    "        except (ValueError, RuntimeError):\n",
    "            print(\"Error , trying again.\")\n",
    "        \n",
    "    rewardsWithConfigs = sampleRewardWithConfig(10**4, setEnv=env, pi_model=pi_model)\n",
    "    modes, proxyRewards = getTopK(rewardsWithConfigs, 200)\n",
    "    roundabouts = getRoundabouts(roadDefinition, modes)\n",
    "    \n",
    "    output[\"roundabouts\"].append(roundabouts)\n",
    "    output[\"modes\"].append(modes)\n",
    "    output[\"proxyRewards\"].append(proxyRewards)\n",
    "    print(log_alpha.exp())\n",
    "    # rewards = getRewards(roundabouts)\n",
    "    # scoresList.append(rewards)\n",
    "import pickle\n",
    "with open('analysis/expSAC_N=4_K=200.pkl', 'wb') as file:\n",
    "    pickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eff25ea7-8700-42c2-b0c9-257219eaa706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan +- nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/ybj42rmx5yzdgsy5vpnr2j980000gn/T/ipykernel_70203/4262500497.py:3: RuntimeWarning: Mean of empty slice.\n",
      "  print(scores.mean(), \"+-\", scores.std())\n",
      "/Users/zarifikram/opt/anaconda3/envs/junction-art/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/zarifikram/opt/anaconda3/envs/junction-art/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/zarifikram/opt/anaconda3/envs/junction-art/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/Users/zarifikram/opt/anaconda3/envs/junction-art/lib/python3.9/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "scores = np.asarray(scoresList)\n",
    "print(scores.mean(), \"+-\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7ef096b8-3c2d-40f6-89c6-6d31e4f3ae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b0c721a-ea2f-4828-b98e-3a41ae65738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8119928845419757 +- 1.4471593257966016\n"
     ]
    }
   ],
   "source": [
    "diversityScores = np.asarray(diversityScores)\n",
    "print(diversityScores.mean(), \"+-\", diversityScores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c87f67da-2a93-47ad-a25f-47585a805137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5475 +- 0.708162234237325\n",
      "0.7975 +- 0.01561249499599601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from junctionart.roundabout.Roundabout import Roundabout\n",
    "from tqdm import tqdm \n",
    "def getRewards(roundabouts):\n",
    "    rewards = []\n",
    "    for roundaboutList in tqdm(roundabouts):\n",
    "        rewardList = [roundabout.getReward() for roundabout in roundaboutList]\n",
    "        rewards.append(rewardList)\n",
    "    return rewards\n",
    "\n",
    "def getDiversityScore(roundabouts):\n",
    "    distances = []\n",
    "    for i in tqdm(range(len(roundabouts))):\n",
    "        for j in range(i + 1, len(roundabouts)):\n",
    "            distance = RewardUtil.getDistance(roundabouts[i], roundabouts[j])\n",
    "            distances.append(distance)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    return distances.sum() / (len(roundabouts) * (len(roundabouts) - 1))\n",
    "\n",
    "roundabouts = output['roundabouts']\n",
    "proxyRewards = output['proxyRewards']\n",
    "# roundabouts = [roundaboutList[:50] for roundaboutList in roundabouts]\n",
    "# proxyRewards = [pList[:50] for pList in proxyRewards]\n",
    "\n",
    "rewards = np.asarray(getRewards(roundabouts))\n",
    "proxyRewards = np.asarray(proxyRewards)\n",
    "\n",
    "print(rewards.mean(), \"+-\", rewards.std())\n",
    "print(proxyRewards.mean(), \"+-\", proxyRewards.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "52ede149-1511-4999-8b31-0b3ea31d642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run episode5999 with rewards 5.011872336272722 s [9.0, 16.0, 26.0, 26.0] ALPHA 0.111\r"
     ]
    }
   ],
   "source": [
    "roadDefinition = roadDefs[0]\n",
    "env = SetGenerationEnv(size, nActions, roadDefinition)\n",
    "\n",
    "targetEntropy = -nActions\n",
    "# log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "# alpha = log_alpha.exp().detach()\n",
    "alpha = 0.1\n",
    "\n",
    "# models\n",
    "pi_model = PolicyNet(state_dim=size, nActions=nActions).to(device)\n",
    "q_origin_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1\n",
    "q_origin_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2\n",
    "q_target_model1 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi1'\n",
    "q_target_model2 = QNet(state_dim=size, nActions=nActions).to(device)  # Q_phi2'\n",
    "_ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "_ = q_target_model2.requires_grad_(False)  # target model doen't need grad\n",
    "buffer = replayBuffer(20000)\n",
    "\n",
    "# optimizers\n",
    "opt_pi = torch.optim.AdamW(pi_model.parameters(), lr=0.0005)\n",
    "opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "# alpha_optimizer = torch.optim.AdamW(params=[log_alpha], lr=0.0005) \n",
    "doneTraining = False\n",
    "\n",
    "train(env, nIter=6000, batch_size=256, disableBar=True)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "17306f27-1822-4bda-a45b-db5f9e73555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    s = torch.zeros(1, size)\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        a = pick_sample((s/nActions).squeeze().tolist(), pi_model)\n",
    "\n",
    "        s_next = env.update(s, torch.tensor([a]), inPlace = False)\n",
    "\n",
    "        done = (s_next != 0).all().item()\n",
    "        if done:\n",
    "            config = (s_next.squeeze() - 1).long().tolist()\n",
    "            r = 10**env.getProxyReward(config, normalize=True)\n",
    "\n",
    "        else:\n",
    "            r = 0\n",
    "        cum_reward += r\n",
    "        s = s_next\n",
    "\n",
    "    # print(f\"state {s.squeeze().tolist()} reward {cum_reward}\")\n",
    "    states.append((s.squeeze().tolist(), cum_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c314965a-401c-4af3-b606-713e4a0fc464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumberOfStates(samples):\n",
    "    sampleCnt = {}\n",
    "    for sample, reward in samples:\n",
    "        if str(sample) in sampleCnt:\n",
    "            sampleCnt[str(sample)] += 1\n",
    "        else:\n",
    "            sampleCnt[str(sample)] = 1\n",
    "    return sampleCnt\n",
    "\n",
    "# samples = sample(agent, 100)\n",
    "len(getNumberOfStates(states))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
