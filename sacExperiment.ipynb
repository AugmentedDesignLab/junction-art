{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5db13d-e83d-450a-a72e-b2eec7ab84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58700919-e2db-4bf1-8f83-93db254b9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# import gym \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfb9a1b-175f-4db7-ba62-cc137160524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Policy net (pi_theta)\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, state_dim = 4, nActions = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(state_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, nActions) # 2 * number + 1\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        return outs\n",
    "\n",
    "# pi_model = PolicyNet(nActions=20).to(device)\n",
    "\n",
    "# Pick up action (for each step in episode)\n",
    "def pick_sample(s, pi_model):\n",
    "    with torch.no_grad():\n",
    "        #   --> size : (1, 4)\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        # Get logits from state\n",
    "        #   --> size : (1, 2)\n",
    "        logits = pi_model(s_batch)\n",
    "        #   --> size : (2)\n",
    "        logits = logits.squeeze(dim=0)\n",
    "        # From logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Pick up action's sample\n",
    "        #   --> size : (1)\n",
    "        a = torch.multinomial(probs, num_samples=1)\n",
    "        #   --> size : ()\n",
    "        a = a.squeeze(dim=0)\n",
    "        # Return\n",
    "        return a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a199666-1191-4f24-9d82-8f876d8c881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim = 4, hidden_dim=512, nActions = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(state_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, nActions) # 2 * number + 1\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86ab67a-f0ff-4ce7-b6f3-082b5a6feb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "# alpha = 0.1\n",
    "\n",
    "\n",
    "class categorical:\n",
    "    def __init__(self, s):\n",
    "        logits = pi_model(s)\n",
    "        self._prob = F.softmax(logits, dim=-1)\n",
    "        self._logp = torch.log(self._prob)\n",
    "\n",
    "    # probability (sum is 1.0) : P\n",
    "    def prob(self):\n",
    "        return self._prob\n",
    "\n",
    "    # log probability : log P()\n",
    "    def logp(self):\n",
    "        return self._logp\n",
    "\n",
    "def optimize_theta(states, alpha):\n",
    "    # Convert to tensor\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    # Disable grad in q_origin_model1 before computation\n",
    "    # (or use q_value.detach() not to include in graph)\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = False\n",
    "    # Optimize\n",
    "    opt_pi.zero_grad()\n",
    "    dist = categorical(states)\n",
    "    q_value = q_origin_model1(states)\n",
    "    term1 = dist.prob()\n",
    "    # alpha = log_alpha.exp().detach()\n",
    "    term2 = q_value - alpha * dist.logp()\n",
    "    # print(term1.shape, term2.shape)\n",
    "    # return\n",
    "    expectation = term1.unsqueeze(dim=1) @ term2.unsqueeze(dim=2)\n",
    "    expectation = expectation.squeeze(dim=1)\n",
    "    (-expectation).sum().backward()\n",
    "    opt_pi.step()\n",
    "    # Enable grad again\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "def optimize_alpha(states):\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    # Disable grad in q_origin_model1 before computation\n",
    "    # (or use q_value.detach() not to include in graph)\n",
    "    for p in pi_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dist = categorical(states)\n",
    "    alpha_optimizer.zero_grad()\n",
    "    alphaLoss = - (log_alpha.exp() * ((dist.logp() * dist.prob()).sum() - targetEntropy)).mean()\n",
    "    alphaLoss.backward()\n",
    "    alpha_optimizer.step()\n",
    "    alpha = log_alpha.exp().detach()\n",
    "    for p in pi_model.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7648d11-7319-4a83-a68e-24e155c01275",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "\n",
    "def optimize_phi(states, actions, rewards, next_states, dones, alpha, nActions=20):\n",
    "    #\n",
    "    # Convert to tensor\n",
    "    #\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    #\n",
    "    # Compute r + gamma * (1 - d) (min Q(s_next,a_next') + alpha * H(P))\n",
    "    #\n",
    "    # alpha = log_alpha.exp().detach()\n",
    "    with torch.no_grad():\n",
    "        # min Q(s_next,a_next')\n",
    "        q1_tgt_next = q_target_model1(next_states)\n",
    "        q2_tgt_next = q_target_model2(next_states)\n",
    "        dist_next = categorical(next_states)\n",
    "        q1_target = q1_tgt_next.unsqueeze(dim=1) @ dist_next.prob().unsqueeze(dim=2)\n",
    "        q1_target = q1_target.squeeze(dim=1)\n",
    "        q2_target = q2_tgt_next.unsqueeze(dim=1) @ dist_next.prob().unsqueeze(dim=2)\n",
    "        q2_target = q2_target.squeeze(dim=1)\n",
    "        q_target_min = torch.minimum(q1_target, q2_target)\n",
    "        # alpha * H(P)\n",
    "        h = dist_next.prob().unsqueeze(dim=1) @ dist_next.logp().unsqueeze(dim=2)\n",
    "        h = h.squeeze(dim=1)\n",
    "        h = -alpha * h\n",
    "        # total\n",
    "        term2 = rewards + gamma * (1.0 - dones) * (q_target_min + h)\n",
    "\n",
    "    #\n",
    "    # Optimize critic loss for Q-network1\n",
    "    #\n",
    "    opt_q1.zero_grad()\n",
    "    one_hot_actions = F.one_hot(actions, num_classes=nActions).float()\n",
    "    q_value1 = q_origin_model1(states)\n",
    "    term1 = q_value1.unsqueeze(dim=1) @ one_hot_actions.unsqueeze(dim=2)\n",
    "    term1 = term1.squeeze(dim=1)\n",
    "    loss_q1 = F.mse_loss(\n",
    "        term1,\n",
    "        term2,\n",
    "        reduction=\"none\")\n",
    "    loss_q1.sum().backward()\n",
    "    opt_q1.step()\n",
    "\n",
    "    #\n",
    "    # Optimize critic loss for Q-network2\n",
    "    #\n",
    "    opt_q2.zero_grad()\n",
    "    one_hot_actions = F.one_hot(actions, num_classes=nActions).float()\n",
    "    q_value2 = q_origin_model2(states)\n",
    "    term1 = q_value2.unsqueeze(dim=1) @ one_hot_actions.unsqueeze(dim=2)\n",
    "    term1 = term1.squeeze(dim=1)\n",
    "    loss_q2 = F.mse_loss(\n",
    "        term1,\n",
    "        term2,\n",
    "        reduction=\"none\")\n",
    "    loss_q2.sum().backward()\n",
    "    opt_q2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37311d8-1a9a-4bfa-80a4-68320a69ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.002\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(q_origin_model1.parameters(), q_target_model1.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(q_origin_model2.parameters(), q_target_model2.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089041d8-1502-4a98-bdda-9ae7b2d3bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) > self._next_idx:\n",
    "            self.buffer[self._next_idx] = item\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "        if self._next_idx == self.buffer_size - 1:\n",
    "            self._next_idx = 0\n",
    "        else:\n",
    "            self._next_idx = self._next_idx + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = [random.randint(0, len(self.buffer) - 1) for _ in range(batch_size)]\n",
    "        states   = [self.buffer[i][0] for i in indices]\n",
    "        actions  = [self.buffer[i][1] for i in indices]\n",
    "        rewards  = [self.buffer[i][2] for i in indices]\n",
    "        n_states = [self.buffer[i][3] for i in indices]\n",
    "        dones    = [self.buffer[i][4] for i in indices]\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "buffer = replayBuffer(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f61b60-c14f-4095-89c9-f7bdae8024fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inputTestCases/_input2ways_n=4_.pickle', 'rb') as f:\n",
    "    roadDefs = pickle.load(f) # deserialize using load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35176975-05d4-4bbc-a2d0-d8fc2b002a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from junctionart.roundabout.encodingGFN.setGenerationEnv2 import SetGenerationEnv2\n",
    "from tqdm import tqdm\n",
    "size = 4\n",
    "nActions = 30\n",
    "\n",
    "\n",
    "def train(env, nIter = 6000, batch_size = 250, disableBar = False):\n",
    "    for i in tqdm(range(nIter), disable = disableBar):\n",
    "        # Run episode till done\n",
    "        s = torch.ones(1, size)\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        while not done:\n",
    "            a = pick_sample((s/nActions).squeeze().tolist(), pi_model)\n",
    "            \n",
    "            done = (a == size)\n",
    "       \n",
    "            if done :\n",
    "                s_next = s\n",
    "            else:\n",
    "                s_next = env.update(s, torch.tensor([a]))\n",
    "            \n",
    "            if done:\n",
    "                config = (s_next.squeeze() - 1).long().tolist()\n",
    "            \n",
    "                r = 10**env.getProxyReward(config, normalize=True) \n",
    "            else:\n",
    "                r = 0\n",
    "            buffer.add([(s/nActions).squeeze().tolist(), a, r, (s_next/nActions).squeeze().tolist(), float(done)])\n",
    "            cum_reward += r\n",
    "            if buffer.length() >= 2000:\n",
    "                states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
    "                optimize_theta(states, alpha)\n",
    "                optimize_alpha(states)\n",
    "                \n",
    "                optimize_phi(states, actions, rewards, n_states, dones, alpha, nActions=size + 1)\n",
    "                update_target()\n",
    "            s = s_next\n",
    "            alpha = log_alpha.exp().detach()\n",
    "        print(\"Run episode{} with rewards {} s {} ALPHA {}\".format(i, cum_reward, s.squeeze().tolist(), alpha), end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0563155d-c1a8-4fdb-aa7d-1b72dcb709c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run episode664 with rewards 3.9810717055349722 s [8.0, 9.0, 7.0, 7.0] ALPHA tensor([0.4032])]))\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6r/ybj42rmx5yzdgsy5vpnr2j980000gn/T/ipykernel_33623/2355678645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdoneTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisableBar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6r/ybj42rmx5yzdgsy5vpnr2j980000gn/T/ipykernel_33623/1153408181.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, nIter, batch_size, disableBar)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcum_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpick_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6r/ybj42rmx5yzdgsy5vpnr2j980000gn/T/ipykernel_33623/1261343840.py\u001b[0m in \u001b[0;36mpick_sample\u001b[0;34m(s, pi_model)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Pick up action's sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#   --> size : (1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#   --> size : ()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "roadDefinition = roadDefs[0]\n",
    "env = SetGenerationEnv2(size, nActions, roadDefinition)\n",
    "\n",
    "targetEntropy = -nActions\n",
    "log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "alpha = log_alpha.exp().detach()\n",
    "\n",
    "# models\n",
    "pi_model = PolicyNet(state_dim=size, nActions=size + 1).to(device)\n",
    "q_origin_model1 = QNet(state_dim=size, nActions=size + 1).to(device)  # Q_phi1\n",
    "q_origin_model2 = QNet(state_dim=size, nActions=size + 1).to(device)  # Q_phi2\n",
    "q_target_model1 = QNet(state_dim=size, nActions=size + 1).to(device)  # Q_phi1'\n",
    "q_target_model2 = QNet(state_dim=size, nActions=size + 1).to(device)  # Q_phi2'\n",
    "_ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "_ = q_target_model2.requires_grad_(False)  # target model doen't need grad\n",
    "buffer = replayBuffer(20000)\n",
    "\n",
    "# optimizers\n",
    "opt_pi = torch.optim.AdamW(pi_model.parameters(), lr=0.0005)\n",
    "opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "alpha_optimizer = torch.optim.AdamW(params=[log_alpha], lr=0.0005) \n",
    "doneTraining = False\n",
    "\n",
    "train(env, nIter=1000, batch_size=256, disableBar=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919717-6b14-4066-b1e3-d2c9a42406f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
